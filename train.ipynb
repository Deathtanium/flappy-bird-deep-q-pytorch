{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "\n",
    "import flappy_bird_gymnasium\n",
    "import gymnasium\n",
    "\n",
    "from deepq_agent import DQNAgent_pytorch\n",
    "\n",
    "from gymnasium.wrappers import FlattenObservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_UPDATE = 10\n",
    "DEVICE = 'cuda' #torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LR = 1e-3\n",
    "GAMMA = 0.99\n",
    "EPS = 1.0\n",
    "EPS_DECAY = 0.0001 \n",
    "EPS_END = 0.1\n",
    "BATCH_SIZE = 128\n",
    "PLAY_MEMORY = 10000\n",
    "LAYERS_SIZES = [64, 64]\n",
    "\n",
    "\n",
    "EPOCHS = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gymnasium.make(\"FlappyBird-v0\")\n",
    "state,_ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get size of observation space\n",
    "obs_space = 11 #len(state) #overriden\n",
    "act_space = env.action_space.n\n",
    "\n",
    "obs_space, act_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent_pytorch(\n",
    "        device=DEVICE,\n",
    "        act_space=act_space,\n",
    "        obs_space=obs_space,\n",
    "        training_batch_size=BATCH_SIZE,\n",
    "        learn_rate=LR,\n",
    "        gamma=GAMMA,\n",
    "        eps=EPS,                                                               #rate of exploration\n",
    "        eps_decay_rate=EPS_DECAY,                                                   \n",
    "        eps_floor=EPS_END,                                                       \n",
    "        network_shape=LAYERS_SIZES,\n",
    "        pmem_buffer_size=PLAY_MEMORY\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent.load(\"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0 Total reward:  4.000000000000002\n",
      "Iteration:  1 Total reward:  4.000000000000002\n",
      "Iteration:  2 Total reward:  4.200000000000001\n",
      "Iteration:  3 Total reward:  3.900000000000002\n",
      "Iteration:  4 Total reward:  4.300000000000001\n",
      "Iteration:  5 Total reward:  4.000000000000002\n",
      "Iteration:  6 Total reward:  4.100000000000001\n",
      "Iteration:  7 Total reward:  4.200000000000001\n",
      "Iteration:  8 Total reward:  4.100000000000001\n",
      "Iteration:  9 Total reward:  4.100000000000001\n",
      "Iteration:  10 Total reward:  4.200000000000001\n",
      "Iteration:  11 Total reward:  4.000000000000002\n",
      "Iteration:  12 Total reward:  4.100000000000001\n",
      "Iteration:  13 Total reward:  4.6\n",
      "Iteration:  14 Total reward:  4.000000000000002\n",
      "Iteration:  15 Total reward:  4.4\n",
      "Iteration:  16 Total reward:  4.300000000000001\n",
      "Iteration:  17 Total reward:  4.300000000000001\n",
      "Iteration:  18 Total reward:  4.300000000000001\n",
      "Iteration:  19 Total reward:  4.799999999999999\n",
      "Iteration:  20 Total reward:  3.900000000000002\n",
      "Iteration:  21 Total reward:  4.799999999999999\n",
      "Iteration:  22 Total reward:  5.4999999999999964\n",
      "Iteration:  23 Total reward:  4.200000000000001\n",
      "Iteration:  24 Total reward:  4.5\n",
      "Iteration:  25 Total reward:  4.200000000000001\n",
      "Iteration:  26 Total reward:  4.5\n",
      "Iteration:  27 Total reward:  4.6\n",
      "Iteration:  28 Total reward:  4.100000000000001\n",
      "Iteration:  29 Total reward:  4.300000000000001\n",
      "Iteration:  30 Total reward:  4.899999999999999\n",
      "Iteration:  31 Total reward:  4.300000000000001\n",
      "Iteration:  32 Total reward:  4.200000000000001\n",
      "Iteration:  33 Total reward:  4.4\n",
      "Iteration:  34 Total reward:  4.6\n",
      "Iteration:  35 Total reward:  4.300000000000001\n",
      "Iteration:  36 Total reward:  4.699999999999999\n",
      "Iteration:  37 Total reward:  4.300000000000001\n",
      "Iteration:  38 Total reward:  4.200000000000001\n",
      "Iteration:  39 Total reward:  4.200000000000001\n",
      "Iteration:  40 Total reward:  4.100000000000001\n",
      "Iteration:  41 Total reward:  4.5\n",
      "Iteration:  42 Total reward:  4.799999999999999\n",
      "Iteration:  43 Total reward:  4.100000000000001\n",
      "Iteration:  44 Total reward:  4.699999999999999\n",
      "Iteration:  45 Total reward:  4.6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ecf-georgem\\Desktop\\flappynn\\flappy-bird-deep-q-pytorch\\train.ipynb Cell 7\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#W6sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     reward \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([reward], device\u001b[39m=\u001b[39mDEVICE)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#W6sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     agent\u001b[39m.\u001b[39mremember(state, action, new_state, reward)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#W6sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     agent\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#W6sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     state \u001b[39m=\u001b[39m new_state\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#W6sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mIteration: \u001b[39m\u001b[39m\"\u001b[39m, i, \u001b[39m\"\u001b[39m\u001b[39mTotal reward: \u001b[39m\u001b[39m\"\u001b[39m, total_reward)\n",
      "File \u001b[1;32mc:\\Users\\ecf-georgem\\Desktop\\flappynn\\flappy-bird-deep-q-pytorch\\deepq_agent.py:126\u001b[0m, in \u001b[0;36mDQNAgent_pytorch.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    124\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m    125\u001b[0m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_value_(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_net\u001b[39m.\u001b[39mparameters(), \u001b[39m1\u001b[39m)\n\u001b[1;32m--> 126\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m    129\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplay_epoch \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    131\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplay_epoch \u001b[39m%\u001b[39m TARGET_UPDATE \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplay_epoch \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ecf-georgem\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    370\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m             )\n\u001b[1;32m--> 373\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    374\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    376\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ecf-georgem\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\ecf-georgem\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adamw.py:184\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    171\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m\"\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    173\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[0;32m    174\u001b[0m         group,\n\u001b[0;32m    175\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    181\u001b[0m         state_steps,\n\u001b[0;32m    182\u001b[0m     )\n\u001b[1;32m--> 184\u001b[0m     adamw(\n\u001b[0;32m    185\u001b[0m         params_with_grad,\n\u001b[0;32m    186\u001b[0m         grads,\n\u001b[0;32m    187\u001b[0m         exp_avgs,\n\u001b[0;32m    188\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    189\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    190\u001b[0m         state_steps,\n\u001b[0;32m    191\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    192\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    193\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    194\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    195\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    196\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    197\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    198\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    199\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    200\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    201\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    202\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    203\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    204\u001b[0m     )\n\u001b[0;32m    206\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\ecf-georgem\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adamw.py:335\u001b[0m, in \u001b[0;36madamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    333\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adamw\n\u001b[1;32m--> 335\u001b[0m func(\n\u001b[0;32m    336\u001b[0m     params,\n\u001b[0;32m    337\u001b[0m     grads,\n\u001b[0;32m    338\u001b[0m     exp_avgs,\n\u001b[0;32m    339\u001b[0m     exp_avg_sqs,\n\u001b[0;32m    340\u001b[0m     max_exp_avg_sqs,\n\u001b[0;32m    341\u001b[0m     state_steps,\n\u001b[0;32m    342\u001b[0m     amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    343\u001b[0m     beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    344\u001b[0m     beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    345\u001b[0m     lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    346\u001b[0m     weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    347\u001b[0m     eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    348\u001b[0m     maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    349\u001b[0m     capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    350\u001b[0m     differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    351\u001b[0m     grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    352\u001b[0m     found_inf\u001b[39m=\u001b[39;49mfound_inf,\n\u001b[0;32m    353\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ecf-georgem\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adamw.py:533\u001b[0m, in \u001b[0;36m_multi_tensor_adamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    530\u001b[0m device_params \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39mview_as_real(x) \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mis_complex(x) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m device_params]\n\u001b[0;32m    532\u001b[0m \u001b[39m# update steps\u001b[39;00m\n\u001b[1;32m--> 533\u001b[0m torch\u001b[39m.\u001b[39;49m_foreach_add_(device_state_steps, \u001b[39m1\u001b[39;49m)\n\u001b[0;32m    535\u001b[0m \u001b[39m# Perform stepweight decay\u001b[39;00m\n\u001b[0;32m    536\u001b[0m \u001b[39mif\u001b[39;00m weight_decay \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gymnasium.make(\"FlappyBird-v0\")\n",
    "\n",
    "env = FlattenObservation(env)\n",
    "\n",
    "def state_filter(state:np.ndarray):\n",
    "    state = state[:-1]\n",
    "    return state\n",
    "\n",
    "import random\n",
    "\n",
    "frames_since_jump = 0\n",
    "for i in range(EPOCHS):\n",
    "    total_reward = 0\n",
    "    state, _ = env.reset(seed=random.randint(0,1000))\n",
    "    state = torch.tensor(state_filter(state), dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "    terminated = False\n",
    "\n",
    "    while not terminated:\n",
    "        action = agent.get_action(state)\n",
    "        \n",
    "        if action.item() == 1:\n",
    "            if frames_since_jump < 5:\n",
    "                action[0] = 0\n",
    "                frames_since_jump += 1\n",
    "            else: \n",
    "                frames_since_jump = 0\n",
    "        else:\n",
    "            frames_since_jump += 1\n",
    "        \n",
    "        new_state, reward, terminated, truncated, info = env.step(action.item())\n",
    "        if terminated or truncated or (new_state[9] < 0 or new_state[9] > 1):\n",
    "            terminated = True\n",
    "            #reward = -1000.0\n",
    "            #reset the environment\n",
    "            state, _ = env.reset()\n",
    "            state = torch.tensor(state_filter(state), dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "        else:\n",
    "            #reward *= 10\n",
    "            pass\n",
    "        \n",
    "        total_reward += reward\n",
    "\n",
    "        new_state = torch.tensor(state_filter(new_state), dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "        reward = torch.tensor([reward], device=DEVICE)\n",
    "        agent.remember(state, action, new_state, reward)\n",
    "        agent.train()\n",
    "\n",
    "        state = new_state\n",
    "    print(\"Iteration: \", i, \"Total reward: \", total_reward)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_hyper(\n",
    "    act_space,\n",
    "    obs_space,\n",
    "    batch_size,\n",
    "    lr,\n",
    "    gamma,\n",
    "    eps_start,\n",
    "    eps_decay,\n",
    "    eps_floor,\n",
    "    layer1_size,\n",
    "    layer2_size,\n",
    "    epochs=100\n",
    "):\n",
    "    #return mean reward over 100 episodes\n",
    "    agent = DQNAgent_pytorch(\n",
    "        act_space=act_space,\n",
    "        obs_space=obs_space,\n",
    "        training_batch_size=batch_size,\n",
    "        learn_rate=lr,\n",
    "        gamma=gamma,\n",
    "        eps=eps_start,\n",
    "        eps_decay_rate=eps_decay,\n",
    "        eps_floor=eps_floor,\n",
    "        network_shape=[layer1_size, layer2_size],\n",
    "        pmem_buffer_size=10000,\n",
    "        device='cuda',\n",
    "    )\n",
    "    env = gymnasium.make(\"FlappyBird-v0\")\n",
    "    state,_ = env.reset()\n",
    "    \n",
    "    total_reward = 0\n",
    "    for i in range(epochs):\n",
    "        state, _ = env.reset()\n",
    "        state = torch.tensor(state_filter(state), dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "        terminated = False\n",
    "\n",
    "        frames_since_jump = 0\n",
    "        while not terminated:\n",
    "            action = agent.get_action(state)\n",
    "            \n",
    "            if action.item() == 1:\n",
    "                if frames_since_jump < 5:\n",
    "                    action[0] = 0\n",
    "                    frames_since_jump += 1\n",
    "                else: \n",
    "                    frames_since_jump = 0\n",
    "            else:\n",
    "                frames_since_jump += 1\n",
    "            \n",
    "            new_state, reward, terminated, truncated, info = env.step(action.item())\n",
    "            if terminated or truncated or (new_state[9] < 0 or new_state[9] > 1):\n",
    "                terminated = True\n",
    "                #reward = -1000.0\n",
    "                #reset the environment\n",
    "                state, _ = env.reset()\n",
    "                state = torch.tensor(state_filter(state), dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "            else:\n",
    "                #reward *= 10\n",
    "                pass\n",
    "            \n",
    "            total_reward += reward\n",
    "\n",
    "            new_state = torch.tensor(state_filter(new_state), dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "            reward = torch.tensor([reward], device=DEVICE)\n",
    "            agent.remember(state, action, new_state, reward)\n",
    "            agent.train()\n",
    "\n",
    "            state = new_state\n",
    "        print(\"Iteration: \", i, \"Total reward: \", total_reward)\n",
    "    return total_reward/epochs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "job exception: local variable 'frames_since_jump' referenced before assignment\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?trial/s, best loss=?]\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'frames_since_jump' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ecf-georgem\\Desktop\\flappynn\\flappy-bird-deep-q-pytorch\\train.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#X11sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39mevaluate_hyper(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#X11sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         act_space\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#X11sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m         obs_space\u001b[39m=\u001b[39m\u001b[39m11\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#X11sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m         epochs\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#X11sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#X11sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m trials \u001b[39m=\u001b[39m hyperopt\u001b[39m.\u001b[39mTrials()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#X11sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m best \u001b[39m=\u001b[39m hyperopt\u001b[39m.\u001b[39;49mfmin(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#X11sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     fn\u001b[39m=\u001b[39;49mobjective,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#X11sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     space\u001b[39m=\u001b[39;49mspace,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#X11sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     algo\u001b[39m=\u001b[39;49mhyperopt\u001b[39m.\u001b[39;49mtpe\u001b[39m.\u001b[39;49msuggest,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#X11sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     max_evals\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#X11sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     trials\u001b[39m=\u001b[39;49mtrials\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#X11sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#X11sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mprint\u001b[39m(best)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#X11sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mprint\u001b[39m(trials\u001b[39m.\u001b[39mbest_trial)\n",
      "File \u001b[1;32mc:\\Users\\ecf-georgem\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\hyperopt\\fmin.py:540\u001b[0m, in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    537\u001b[0m     fn \u001b[39m=\u001b[39m __objective_fmin_wrapper(fn)\n\u001b[0;32m    539\u001b[0m \u001b[39mif\u001b[39;00m allow_trials_fmin \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(trials, \u001b[39m\"\u001b[39m\u001b[39mfmin\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 540\u001b[0m     \u001b[39mreturn\u001b[39;00m trials\u001b[39m.\u001b[39;49mfmin(\n\u001b[0;32m    541\u001b[0m         fn,\n\u001b[0;32m    542\u001b[0m         space,\n\u001b[0;32m    543\u001b[0m         algo\u001b[39m=\u001b[39;49malgo,\n\u001b[0;32m    544\u001b[0m         max_evals\u001b[39m=\u001b[39;49mmax_evals,\n\u001b[0;32m    545\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    546\u001b[0m         loss_threshold\u001b[39m=\u001b[39;49mloss_threshold,\n\u001b[0;32m    547\u001b[0m         max_queue_len\u001b[39m=\u001b[39;49mmax_queue_len,\n\u001b[0;32m    548\u001b[0m         rstate\u001b[39m=\u001b[39;49mrstate,\n\u001b[0;32m    549\u001b[0m         pass_expr_memo_ctrl\u001b[39m=\u001b[39;49mpass_expr_memo_ctrl,\n\u001b[0;32m    550\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    551\u001b[0m         catch_eval_exceptions\u001b[39m=\u001b[39;49mcatch_eval_exceptions,\n\u001b[0;32m    552\u001b[0m         return_argmin\u001b[39m=\u001b[39;49mreturn_argmin,\n\u001b[0;32m    553\u001b[0m         show_progressbar\u001b[39m=\u001b[39;49mshow_progressbar,\n\u001b[0;32m    554\u001b[0m         early_stop_fn\u001b[39m=\u001b[39;49mearly_stop_fn,\n\u001b[0;32m    555\u001b[0m         trials_save_file\u001b[39m=\u001b[39;49mtrials_save_file,\n\u001b[0;32m    556\u001b[0m     )\n\u001b[0;32m    558\u001b[0m \u001b[39mif\u001b[39;00m trials \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    559\u001b[0m     \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(trials_save_file):\n",
      "File \u001b[1;32mc:\\Users\\ecf-georgem\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\hyperopt\\base.py:671\u001b[0m, in \u001b[0;36mTrials.fmin\u001b[1;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[39m# -- Stop-gap implementation!\u001b[39;00m\n\u001b[0;32m    667\u001b[0m \u001b[39m#    fmin should have been a Trials method in the first place\u001b[39;00m\n\u001b[0;32m    668\u001b[0m \u001b[39m#    but for now it's still sitting in another file.\u001b[39;00m\n\u001b[0;32m    669\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfmin\u001b[39;00m \u001b[39mimport\u001b[39;00m fmin\n\u001b[1;32m--> 671\u001b[0m \u001b[39mreturn\u001b[39;00m fmin(\n\u001b[0;32m    672\u001b[0m     fn,\n\u001b[0;32m    673\u001b[0m     space,\n\u001b[0;32m    674\u001b[0m     algo\u001b[39m=\u001b[39;49malgo,\n\u001b[0;32m    675\u001b[0m     max_evals\u001b[39m=\u001b[39;49mmax_evals,\n\u001b[0;32m    676\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    677\u001b[0m     loss_threshold\u001b[39m=\u001b[39;49mloss_threshold,\n\u001b[0;32m    678\u001b[0m     trials\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    679\u001b[0m     rstate\u001b[39m=\u001b[39;49mrstate,\n\u001b[0;32m    680\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    681\u001b[0m     max_queue_len\u001b[39m=\u001b[39;49mmax_queue_len,\n\u001b[0;32m    682\u001b[0m     allow_trials_fmin\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,  \u001b[39m# -- prevent recursion\u001b[39;49;00m\n\u001b[0;32m    683\u001b[0m     pass_expr_memo_ctrl\u001b[39m=\u001b[39;49mpass_expr_memo_ctrl,\n\u001b[0;32m    684\u001b[0m     catch_eval_exceptions\u001b[39m=\u001b[39;49mcatch_eval_exceptions,\n\u001b[0;32m    685\u001b[0m     return_argmin\u001b[39m=\u001b[39;49mreturn_argmin,\n\u001b[0;32m    686\u001b[0m     show_progressbar\u001b[39m=\u001b[39;49mshow_progressbar,\n\u001b[0;32m    687\u001b[0m     early_stop_fn\u001b[39m=\u001b[39;49mearly_stop_fn,\n\u001b[0;32m    688\u001b[0m     trials_save_file\u001b[39m=\u001b[39;49mtrials_save_file,\n\u001b[0;32m    689\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ecf-georgem\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\hyperopt\\fmin.py:586\u001b[0m, in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    583\u001b[0m rval\u001b[39m.\u001b[39mcatch_eval_exceptions \u001b[39m=\u001b[39m catch_eval_exceptions\n\u001b[0;32m    585\u001b[0m \u001b[39m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[1;32m--> 586\u001b[0m rval\u001b[39m.\u001b[39;49mexhaust()\n\u001b[0;32m    588\u001b[0m \u001b[39mif\u001b[39;00m return_argmin:\n\u001b[0;32m    589\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(trials\u001b[39m.\u001b[39mtrials) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ecf-georgem\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\hyperopt\\fmin.py:364\u001b[0m, in \u001b[0;36mFMinIter.exhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexhaust\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    363\u001b[0m     n_done \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials)\n\u001b[1;32m--> 364\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_evals \u001b[39m-\u001b[39;49m n_done, block_until_done\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49masynchronous)\n\u001b[0;32m    365\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials\u001b[39m.\u001b[39mrefresh()\n\u001b[0;32m    366\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ecf-georgem\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\hyperopt\\fmin.py:300\u001b[0m, in \u001b[0;36mFMinIter.run\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    297\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpoll_interval_secs)\n\u001b[0;32m    298\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    299\u001b[0m     \u001b[39m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[1;32m--> 300\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mserial_evaluate()\n\u001b[0;32m    302\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials\u001b[39m.\u001b[39mrefresh()\n\u001b[0;32m    303\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials_save_file \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ecf-georgem\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\hyperopt\\fmin.py:178\u001b[0m, in \u001b[0;36mFMinIter.serial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    176\u001b[0m ctrl \u001b[39m=\u001b[39m base\u001b[39m.\u001b[39mCtrl(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials, current_trial\u001b[39m=\u001b[39mtrial)\n\u001b[0;32m    177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdomain\u001b[39m.\u001b[39;49mevaluate(spec, ctrl)\n\u001b[0;32m    179\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    180\u001b[0m     logger\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mjob exception: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mstr\u001b[39m(e))\n",
      "File \u001b[1;32mc:\\Users\\ecf-georgem\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\hyperopt\\base.py:892\u001b[0m, in \u001b[0;36mDomain.evaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    884\u001b[0m     \u001b[39m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[0;32m    885\u001b[0m     \u001b[39m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[0;32m    886\u001b[0m     \u001b[39m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[0;32m    887\u001b[0m     pyll_rval \u001b[39m=\u001b[39m pyll\u001b[39m.\u001b[39mrec_eval(\n\u001b[0;32m    888\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexpr,\n\u001b[0;32m    889\u001b[0m         memo\u001b[39m=\u001b[39mmemo,\n\u001b[0;32m    890\u001b[0m         print_node_on_error\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrec_eval_print_node_on_error,\n\u001b[0;32m    891\u001b[0m     )\n\u001b[1;32m--> 892\u001b[0m     rval \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(pyll_rval)\n\u001b[0;32m    894\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(rval, (\u001b[39mfloat\u001b[39m, \u001b[39mint\u001b[39m, np\u001b[39m.\u001b[39mnumber)):\n\u001b[0;32m    895\u001b[0m     dict_rval \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mfloat\u001b[39m(rval), \u001b[39m\"\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m\"\u001b[39m: STATUS_OK}\n",
      "\u001b[1;32mc:\\Users\\ecf-georgem\\Desktop\\flappynn\\flappy-bird-deep-q-pytorch\\train.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#X11sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mobjective\u001b[39m(params):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#X11sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39mevaluate_hyper(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#X11sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         act_space\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#X11sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m         obs_space\u001b[39m=\u001b[39;49m\u001b[39m11\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#X11sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m         batch_size\u001b[39m=\u001b[39;49mparams[\u001b[39m'\u001b[39;49m\u001b[39mbatch_size\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#X11sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m         lr\u001b[39m=\u001b[39;49mparams[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#X11sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m         gamma\u001b[39m=\u001b[39;49mparams[\u001b[39m'\u001b[39;49m\u001b[39mgamma\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#X11sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m         eps_start\u001b[39m=\u001b[39;49mparams[\u001b[39m'\u001b[39;49m\u001b[39meps_start\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#X11sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m         eps_decay\u001b[39m=\u001b[39;49mparams[\u001b[39m'\u001b[39;49m\u001b[39meps_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#X11sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m         eps_floor\u001b[39m=\u001b[39;49mparams[\u001b[39m'\u001b[39;49m\u001b[39meps_floor\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#X11sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m         layer1_size\u001b[39m=\u001b[39;49mparams[\u001b[39m'\u001b[39;49m\u001b[39mlayer1_size\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#X11sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m         layer2_size\u001b[39m=\u001b[39;49mparams[\u001b[39m'\u001b[39;49m\u001b[39mlayer2_size\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#X11sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m         epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#X11sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     )\n",
      "\u001b[1;32mc:\\Users\\ecf-georgem\\Desktop\\flappynn\\flappy-bird-deep-q-pytorch\\train.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#X11sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mget_action(state)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#X11sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mif\u001b[39;00m action\u001b[39m.\u001b[39mitem() \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#X11sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     \u001b[39mif\u001b[39;00m frames_since_jump \u001b[39m<\u001b[39m \u001b[39m5\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#X11sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m         action[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#X11sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m         frames_since_jump \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'frames_since_jump' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import hyperopt\n",
    "\n",
    "batch_sizes = [32, 64, 128, 256, 512]\n",
    "lrs = [1e-2, 1e-3, 1e-4, 1e-5]\n",
    "gammas = [0.99]\n",
    "eps_starts = [1.0]\n",
    "eps_decays = [0.0001]\n",
    "eps_floors = [0.01] \n",
    "layer1_sizes = [16, 32, 64, 128]\n",
    "layer2_sizes = [16, 32, 64, 128]\n",
    "\n",
    "space = {\n",
    "    'batch_size': hyperopt.hp.choice('batch_size', batch_sizes),\n",
    "    'lr': hyperopt.hp.choice('lr', lrs),\n",
    "    'gamma': hyperopt.hp.choice('gamma', gammas),\n",
    "    'eps_start': hyperopt.hp.choice('eps_start', eps_starts),\n",
    "    'eps_decay': hyperopt.hp.choice('eps_decay', eps_decays),\n",
    "    'eps_floor': hyperopt.hp.choice('eps_floor', eps_floors),\n",
    "    'layer1_size': hyperopt.hp.choice('layer1_size', layer1_sizes),\n",
    "    'layer2_size': hyperopt.hp.choice('layer2_size', layer2_sizes),\n",
    "}\n",
    "\n",
    "def objective(params):\n",
    "    return -evaluate_hyper(\n",
    "        act_space=2,\n",
    "        obs_space=11,\n",
    "        batch_size=params['batch_size'],\n",
    "        lr=params['lr'],\n",
    "        gamma=params['gamma'],\n",
    "        eps_start=params['eps_start'],\n",
    "        eps_decay=params['eps_decay'],\n",
    "        eps_floor=params['eps_floor'],\n",
    "        layer1_size=params['layer1_size'],\n",
    "        layer2_size=params['layer2_size'],\n",
    "        epochs=100\n",
    "    )\n",
    "\n",
    "trials = hyperopt.Trials()\n",
    "best = hyperopt.fmin(\n",
    "    fn=objective,\n",
    "    space=space,\n",
    "    algo=hyperopt.tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "print(best)\n",
    "print(trials.best_trial)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flappynn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
