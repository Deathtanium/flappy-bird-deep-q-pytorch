{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "\n",
    "import flappy_bird_gymnasium\n",
    "import gymnasium\n",
    "\n",
    "from deepq_agent import DQNAgent_pytorch\n",
    "\n",
    "from gymnasium.wrappers import FlattenObservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = \"CartPole-v1\" #CartPole-v1 FlappyBird-v0\n",
    "env = gymnasium.make(game) \n",
    "state,_ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get size of observation space\n",
    "obs_space = len(state) #overriden\n",
    "act_space = env.action_space.n\n",
    "\n",
    "obs_space, act_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_UPDATE = 100\n",
    "DEVICE = 'cuda' #torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LR = 1e-4\n",
    "GAMMA = 0.99\n",
    "EPS = 0.9\n",
    "EPS_DECAY = 1000 \n",
    "EPS_END = 0.05\n",
    "BATCH_SIZE = 128\n",
    "PLAY_MEMORY = 10000\n",
    "LAYERS_SIZES = [256, 256]\n",
    "EPOCHS = 1000000\n",
    "TAU = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent_pytorch(\n",
    "        device=DEVICE,\n",
    "        act_space=act_space,\n",
    "        obs_space=obs_space,\n",
    "        training_batch_size=BATCH_SIZE,\n",
    "        learn_rate=LR,\n",
    "        gamma=GAMMA,\n",
    "        eps_start=EPS,                                                               #rate of exploration\n",
    "        eps_decay_rate=EPS_DECAY,                                                   \n",
    "        eps_floor=EPS_END,                                                       \n",
    "        network_shape=LAYERS_SIZES,\n",
    "        tau=TAU,\n",
    "        pmem_buffer_size=PLAY_MEMORY\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent.load(\"model.pt\")\n",
    "def state_filter(state:np.ndarray):\n",
    "    state = state#[:-1]\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0 Total reward:  26\n",
      "Iteration:  1 Total reward:  44\n",
      "Iteration:  2 Total reward:  23\n",
      "Iteration:  3 Total reward:  33\n",
      "Iteration:  4 Total reward:  40\n",
      "Iteration:  5 Total reward:  15\n",
      "Iteration:  6 Total reward:  20\n",
      "Iteration:  7 Total reward:  16\n",
      "Iteration:  8 Total reward:  18\n",
      "Iteration:  9 Total reward:  12\n",
      "Iteration:  10 Total reward:  9\n",
      "Iteration:  11 Total reward:  9\n",
      "Iteration:  12 Total reward:  12\n",
      "Iteration:  13 Total reward:  9\n",
      "Iteration:  14 Total reward:  19\n",
      "Iteration:  15 Total reward:  11\n",
      "Iteration:  16 Total reward:  16\n",
      "Iteration:  17 Total reward:  16\n",
      "Iteration:  18 Total reward:  18\n",
      "Iteration:  19 Total reward:  23\n",
      "Iteration:  20 Total reward:  11\n",
      "Iteration:  21 Total reward:  11\n",
      "Iteration:  22 Total reward:  17\n",
      "Iteration:  23 Total reward:  8\n",
      "Iteration:  24 Total reward:  13\n",
      "Iteration:  25 Total reward:  13\n",
      "Iteration:  26 Total reward:  11\n",
      "Iteration:  27 Total reward:  9\n",
      "Iteration:  28 Total reward:  9\n",
      "Iteration:  29 Total reward:  11\n",
      "Iteration:  30 Total reward:  13\n",
      "Iteration:  31 Total reward:  13\n",
      "Iteration:  32 Total reward:  8\n",
      "Iteration:  33 Total reward:  10\n",
      "Iteration:  34 Total reward:  8\n",
      "Iteration:  35 Total reward:  20\n",
      "Iteration:  36 Total reward:  10\n",
      "Iteration:  37 Total reward:  12\n",
      "Iteration:  38 Total reward:  14\n",
      "Iteration:  39 Total reward:  12\n",
      "Iteration:  40 Total reward:  7\n",
      "Iteration:  41 Total reward:  7\n",
      "Iteration:  42 Total reward:  19\n",
      "Iteration:  43 Total reward:  12\n",
      "Iteration:  44 Total reward:  29\n",
      "Iteration:  45 Total reward:  12\n",
      "Iteration:  46 Total reward:  18\n",
      "Iteration:  47 Total reward:  26\n",
      "Iteration:  48 Total reward:  22\n",
      "Iteration:  49 Total reward:  52\n",
      "Iteration:  50 Total reward:  15\n",
      "Iteration:  51 Total reward:  18\n",
      "Iteration:  52 Total reward:  28\n",
      "Iteration:  53 Total reward:  72\n",
      "Iteration:  54 Total reward:  124\n",
      "Iteration:  55 Total reward:  23\n",
      "Iteration:  56 Total reward:  36\n",
      "Iteration:  57 Total reward:  132\n",
      "Iteration:  58 Total reward:  129\n",
      "Iteration:  59 Total reward:  150\n",
      "Iteration:  60 Total reward:  121\n",
      "Iteration:  61 Total reward:  156\n",
      "Iteration:  62 Total reward:  183\n",
      "Iteration:  63 Total reward:  170\n",
      "Iteration:  64 Total reward:  197\n",
      "Iteration:  65 Total reward:  236\n",
      "Iteration:  66 Total reward:  263\n",
      "Iteration:  67 Total reward:  189\n",
      "Iteration:  68 Total reward:  189\n",
      "Iteration:  69 Total reward:  295\n",
      "Iteration:  70 Total reward:  232\n",
      "Iteration:  71 Total reward:  269\n",
      "Iteration:  72 Total reward:  337\n",
      "Iteration:  73 Total reward:  209\n",
      "Iteration:  74 Total reward:  192\n",
      "Iteration:  75 Total reward:  244\n",
      "Iteration:  76 Total reward:  164\n",
      "Iteration:  77 Total reward:  235\n",
      "Iteration:  78 Total reward:  188\n",
      "Iteration:  79 Total reward:  193\n",
      "Iteration:  80 Total reward:  181\n",
      "Iteration:  81 Total reward:  192\n",
      "Iteration:  82 Total reward:  190\n",
      "Iteration:  83 Total reward:  228\n",
      "Iteration:  84 Total reward:  172\n",
      "Iteration:  85 Total reward:  218\n",
      "Iteration:  86 Total reward:  226\n",
      "Iteration:  87 Total reward:  217\n",
      "Iteration:  88 Total reward:  225\n",
      "Iteration:  89 Total reward:  191\n",
      "Iteration:  90 Total reward:  201\n",
      "Iteration:  91 Total reward:  204\n",
      "Iteration:  92 Total reward:  165\n",
      "Iteration:  93 Total reward:  205\n",
      "Iteration:  94 Total reward:  244\n",
      "Iteration:  95 Total reward:  231\n",
      "Iteration:  96 Total reward:  260\n",
      "Iteration:  97 Total reward:  252\n",
      "Iteration:  98 Total reward:  232\n",
      "Iteration:  99 Total reward:  255\n",
      "Iteration:  100 Total reward:  248\n",
      "Iteration:  101 Total reward:  272\n",
      "Iteration:  102 Total reward:  263\n",
      "Iteration:  103 Total reward:  233\n",
      "Iteration:  104 Total reward:  270\n",
      "Iteration:  105 Total reward:  259\n",
      "Iteration:  106 Total reward:  245\n",
      "Iteration:  107 Total reward:  226\n",
      "Iteration:  108 Total reward:  268\n",
      "Iteration:  109 Total reward:  261\n",
      "Iteration:  110 Total reward:  320\n",
      "Iteration:  111 Total reward:  284\n",
      "Iteration:  112 Total reward:  265\n",
      "Iteration:  113 Total reward:  294\n",
      "Iteration:  114 Total reward:  301\n",
      "Iteration:  115 Total reward:  270\n",
      "Iteration:  116 Total reward:  289\n",
      "Iteration:  117 Total reward:  278\n",
      "Iteration:  118 Total reward:  239\n",
      "Iteration:  119 Total reward:  265\n",
      "Iteration:  120 Total reward:  294\n",
      "Iteration:  121 Total reward:  254\n",
      "Iteration:  122 Total reward:  257\n",
      "Iteration:  123 Total reward:  258\n",
      "Iteration:  124 Total reward:  263\n",
      "Iteration:  125 Total reward:  295\n",
      "Iteration:  126 Total reward:  299\n",
      "Iteration:  127 Total reward:  266\n",
      "Iteration:  128 Total reward:  312\n",
      "Iteration:  129 Total reward:  269\n",
      "Iteration:  130 Total reward:  339\n",
      "Iteration:  131 Total reward:  290\n",
      "Iteration:  132 Total reward:  362\n",
      "Iteration:  133 Total reward:  285\n",
      "Iteration:  134 Total reward:  313\n",
      "Iteration:  135 Total reward:  312\n",
      "Iteration:  136 Total reward:  468\n",
      "Iteration:  137 Total reward:  440\n",
      "Iteration:  138 Total reward:  456\n",
      "Iteration:  139 Total reward:  432\n",
      "Iteration:  140 Total reward:  499\n",
      "Iteration:  141 Total reward:  310\n",
      "Iteration:  142 Total reward:  421\n",
      "Iteration:  143 Total reward:  373\n",
      "Iteration:  144 Total reward:  422\n",
      "Iteration:  145 Total reward:  485\n",
      "Iteration:  146 Total reward:  373\n",
      "Iteration:  147 Total reward:  366\n",
      "Iteration:  148 Total reward:  248\n",
      "Iteration:  149 Total reward:  233\n",
      "Iteration:  150 Total reward:  499\n",
      "Iteration:  151 Total reward:  243\n",
      "Iteration:  152 Total reward:  211\n",
      "Iteration:  153 Total reward:  326\n",
      "Iteration:  154 Total reward:  214\n",
      "Iteration:  155 Total reward:  162\n",
      "Iteration:  156 Total reward:  162\n",
      "Iteration:  157 Total reward:  318\n",
      "Iteration:  158 Total reward:  164\n",
      "Iteration:  159 Total reward:  171\n",
      "Iteration:  160 Total reward:  177\n",
      "Iteration:  161 Total reward:  176\n",
      "Iteration:  162 Total reward:  157\n",
      "Iteration:  163 Total reward:  220\n",
      "Iteration:  164 Total reward:  168\n",
      "Iteration:  165 Total reward:  150\n",
      "Iteration:  166 Total reward:  253\n",
      "Iteration:  167 Total reward:  171\n",
      "Iteration:  168 Total reward:  188\n",
      "Iteration:  169 Total reward:  162\n",
      "Iteration:  170 Total reward:  189\n",
      "Iteration:  171 Total reward:  140\n",
      "Iteration:  172 Total reward:  179\n",
      "Iteration:  173 Total reward:  169\n",
      "Iteration:  174 Total reward:  227\n",
      "Iteration:  175 Total reward:  187\n",
      "Iteration:  176 Total reward:  179\n",
      "Iteration:  177 Total reward:  180\n",
      "Iteration:  178 Total reward:  209\n",
      "Iteration:  179 Total reward:  171\n",
      "Iteration:  180 Total reward:  181\n",
      "Iteration:  181 Total reward:  180\n",
      "Iteration:  182 Total reward:  271\n",
      "Iteration:  183 Total reward:  174\n",
      "Iteration:  184 Total reward:  195\n",
      "Iteration:  185 Total reward:  158\n",
      "Iteration:  186 Total reward:  184\n",
      "Iteration:  187 Total reward:  145\n",
      "Iteration:  188 Total reward:  151\n",
      "Iteration:  189 Total reward:  184\n",
      "Iteration:  190 Total reward:  143\n",
      "Iteration:  191 Total reward:  195\n",
      "Iteration:  192 Total reward:  166\n",
      "Iteration:  193 Total reward:  184\n",
      "Iteration:  194 Total reward:  194\n",
      "Iteration:  195 Total reward:  238\n",
      "Iteration:  196 Total reward:  244\n",
      "Iteration:  197 Total reward:  246\n",
      "Iteration:  198 Total reward:  250\n",
      "Iteration:  199 Total reward:  303\n",
      "Iteration:  200 Total reward:  488\n",
      "Iteration:  201 Total reward:  297\n",
      "Iteration:  202 Total reward:  261\n",
      "Iteration:  203 Total reward:  310\n",
      "Iteration:  204 Total reward:  333\n",
      "Iteration:  205 Total reward:  333\n",
      "Iteration:  206 Total reward:  320\n",
      "Iteration:  207 Total reward:  316\n",
      "Iteration:  208 Total reward:  370\n",
      "Iteration:  209 Total reward:  226\n",
      "Iteration:  210 Total reward:  345\n",
      "Iteration:  211 Total reward:  424\n",
      "Iteration:  212 Total reward:  436\n",
      "Iteration:  213 Total reward:  442\n",
      "Iteration:  214 Total reward:  499\n",
      "Iteration:  215 Total reward:  499\n",
      "Iteration:  216 Total reward:  499\n",
      "Iteration:  217 Total reward:  499\n",
      "Iteration:  218 Total reward:  499\n",
      "Iteration:  219 Total reward:  377\n",
      "Iteration:  220 Total reward:  499\n",
      "Iteration:  221 Total reward:  499\n",
      "Iteration:  222 Total reward:  499\n",
      "Iteration:  223 Total reward:  499\n",
      "Iteration:  224 Total reward:  499\n",
      "Iteration:  225 Total reward:  272\n",
      "Iteration:  226 Total reward:  254\n",
      "Iteration:  227 Total reward:  499\n",
      "Iteration:  228 Total reward:  499\n",
      "Iteration:  229 Total reward:  499\n",
      "Iteration:  230 Total reward:  499\n",
      "Iteration:  231 Total reward:  499\n",
      "Iteration:  232 Total reward:  499\n",
      "Iteration:  233 Total reward:  499\n",
      "Iteration:  234 Total reward:  499\n",
      "Iteration:  235 Total reward:  499\n",
      "Iteration:  236 Total reward:  499\n",
      "Iteration:  237 Total reward:  499\n",
      "Iteration:  238 Total reward:  499\n",
      "Iteration:  239 Total reward:  499\n",
      "Iteration:  240 Total reward:  499\n",
      "Iteration:  241 Total reward:  499\n",
      "Iteration:  242 Total reward:  499\n",
      "Iteration:  243 Total reward:  499\n",
      "Iteration:  244 Total reward:  499\n",
      "Iteration:  245 Total reward:  499\n",
      "Iteration:  246 Total reward:  499\n",
      "Iteration:  247 Total reward:  34\n",
      "Iteration:  248 Total reward:  499\n",
      "Iteration:  249 Total reward:  499\n",
      "Iteration:  250 Total reward:  499\n",
      "Iteration:  251 Total reward:  499\n",
      "Iteration:  252 Total reward:  499\n",
      "Iteration:  253 Total reward:  499\n",
      "Iteration:  254 Total reward:  499\n",
      "Iteration:  255 Total reward:  499\n",
      "Iteration:  256 Total reward:  499\n",
      "Iteration:  257 Total reward:  499\n",
      "Iteration:  258 Total reward:  499\n",
      "Iteration:  259 Total reward:  499\n",
      "Iteration:  260 Total reward:  499\n",
      "Iteration:  261 Total reward:  499\n",
      "Iteration:  262 Total reward:  499\n",
      "Iteration:  263 Total reward:  499\n",
      "Iteration:  264 Total reward:  499\n",
      "Iteration:  265 Total reward:  499\n",
      "Iteration:  266 Total reward:  499\n",
      "Iteration:  267 Total reward:  499\n",
      "Iteration:  268 Total reward:  499\n",
      "Iteration:  269 Total reward:  499\n",
      "Iteration:  270 Total reward:  499\n",
      "Iteration:  271 Total reward:  499\n",
      "Iteration:  272 Total reward:  499\n",
      "Iteration:  273 Total reward:  499\n",
      "Iteration:  274 Total reward:  499\n",
      "Iteration:  275 Total reward:  499\n",
      "Iteration:  276 Total reward:  499\n",
      "Iteration:  277 Total reward:  499\n",
      "Iteration:  278 Total reward:  499\n",
      "Iteration:  279 Total reward:  499\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ecf-georgem\\Desktop\\flappynn\\flappy-bird-deep-q-pytorch\\train.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#W6sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Perform one step of the optimization (on the policy network)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#W6sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m agent\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#W6sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m total_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\u001b[39m.\u001b[39;49mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#W6sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# Soft update of the target network's weights\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#W6sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# θ′ ← τ θ + (1 −τ )θ′\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#W6sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m policy_dict \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mpolicy_net\u001b[39m.\u001b[39mstate_dict()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from itertools import count\n",
    "import random\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    state, _ = env.reset()\n",
    "    state = torch.tensor(state_filter(state), dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "\n",
    "    total_reward = 0\n",
    "\n",
    "    for t in count():\n",
    "        action = agent.get_action(state, env.action_space)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=DEVICE)\n",
    "        next_state = torch.tensor(state_filter(next_state), dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "        done = terminated or truncated\n",
    "\n",
    "\n",
    "        # Store the transition in memory    \n",
    "        agent.memory.push(state, action, next_state, reward, torch.tensor([done], device=DEVICE))\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        agent.train()\n",
    "\n",
    "        total_reward += reward.item()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        policy_dict = agent.policy_net.state_dict()\n",
    "        target_dict = agent.target_net.state_dict()\n",
    "        for key in policy_dict:\n",
    "            target_dict[key] = agent.tau * policy_dict[key] + (1 - agent.tau) * target_dict[key]\n",
    "        agent.target_net.load_state_dict(target_dict)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    print(\"Iteration: \", epoch, \"Total reward: \", t )\n",
    "    agent.save(\"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0 Total reward:  499\n",
      "Iteration:  1 Total reward:  499\n",
      "Iteration:  2 Total reward:  499\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ecf-georgem\\Desktop\\flappynn\\flappy-bird-deep-q-pytorch\\train.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m count():\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mget_action(state, env\u001b[39m.\u001b[39maction_space)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     next_state, reward, terminated, truncated, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action\u001b[39m.\u001b[39;49mitem())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     reward \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([reward], device\u001b[39m=\u001b[39mDEVICE)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ecf-georgem/Desktop/flappynn/flappy-bird-deep-q-pytorch/train.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     next_state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(state_filter(next_state), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32, device\u001b[39m=\u001b[39mDEVICE)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ecf-georgem\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\wrappers\\time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     47\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     58\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     60\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Users\\ecf-georgem\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\ecf-georgem\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\ecf-georgem\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\envs\\classic_control\\cartpole.py:190\u001b[0m, in \u001b[0;36mCartPoleEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    187\u001b[0m     reward \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m    189\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 190\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender()\n\u001b[0;32m    191\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32), reward, terminated, \u001b[39mFalse\u001b[39;00m, {}\n",
      "File \u001b[1;32mc:\\Users\\ecf-georgem\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\envs\\classic_control\\cartpole.py:302\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    301\u001b[0m     pygame\u001b[39m.\u001b[39mevent\u001b[39m.\u001b[39mpump()\n\u001b[1;32m--> 302\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclock\u001b[39m.\u001b[39;49mtick(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetadata[\u001b[39m\"\u001b[39;49m\u001b[39mrender_fps\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m    303\u001b[0m     pygame\u001b[39m.\u001b[39mdisplay\u001b[39m.\u001b[39mflip()\n\u001b[0;32m    305\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrgb_array\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.load(\"modelstick.pt\")\n",
    "env = gymnasium.make(game, render_mode=\"human\")\n",
    "state, _ = env.reset()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    state, _ = env.reset()\n",
    "    state = torch.tensor(state_filter(state), dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "\n",
    "    for t in count():\n",
    "        action = agent.get_action(state, env.action_space)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=DEVICE)\n",
    "        next_state = torch.tensor(state_filter(next_state), dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "        done = terminated or truncated\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    print(\"Iteration: \", epoch, \"Total reward: \", t)\n",
    "    agent.save(\"model.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flappynn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
